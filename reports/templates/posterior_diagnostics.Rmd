---
title: "Posterior diagnostics for drone data"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
      number_sections: true
params:
  nim_pkg: output/mcmc/nim_pkg_mns.rds
  samples: output/mcmc/fit_mns.rds
  nburn: 5e3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE, message = FALSE, 
                      warning = FALSE)
```

Posterior Diagnostic Report to help with interpretation of data and measurements that are input into the model.  
by KC Bierlich  
Updated: 07/26/21  
  
  
# Configuration

```{r load_model, include = FALSE}
library(nimble)

## Load R files and subplans
lapply(list.files("../../R", full.names = TRUE, recursive = TRUE), source)

# load inputs to target model
nim_pkg = readRDS(file.path('..', '..', params$nim_pkg))

# build nimble model, so we can extract priors
droneLengths = nimbleModel(code = model, constants = nim_pkg$consts,
                           data = nim_pkg$data, inits = nim_pkg$inits,
                           name = 'droneLengths')
```

```{r load_samples, include = FALSE}
samples = readRDS(file.path('..', '..', params$samples))   # name of samples - measurements, altitude
samples.names = colnames(samples)                          # their names, i.e., L[1], a[1], bias[1], sigma[1]
burn = 1:params$nburn
```

This report was generated by the target 
<span style="color:blue">`r id_chr()`</span>, which displays
posterior diagnostics for the samples in 
<span style="color:blue">`r basename(params$samples)`</span>.  The file 
contains <span style="color:blue">`r format(nrow(samples), big.mark = ',')`</span> posterior samples.


# Posterior learning 

## Tabs {.tabset}

### Summary table

```{r summary_table}
library(coda)

tgt = c(
  paste('sigma[', 1:3, ']', sep = '')
)

m = mcmc(samples[-burn, tgt])
s = summary(m)

round(s$statistics, 2)

round(HPDinterval(m), 2)

```


```{r correlations}
# posterior correlations
m.cor = cor(m)

# full matrix
round(m.cor, 2)
```

```{r prior_post_plot_fn, include = FALSE}
library(ggplot2)
library(ggthemes)
library(coda)

distn_info = function(pattern, model = NULL, samples, burn, 
                      output = c('prior', 'prior_post', 'trace')) {
  # Parameters:
  #  pattern - string to select nodes
  #  model - nimble model, needed to automatically extract priors
  #  samples - posterior samples
  #  burn - number of samples to discard
  
  # extract model nodes that match the pattern
  tgt = grep(pattern = pattern, x = colnames(samples), value = TRUE)
  
  if(length(burn) == 1) {
    burn = 1:burn
  }
  
  # process matching nodes
  res = lapply(tgt, function(node) {
    
    # initialize output for node
    res = list()
    
    if(TRUE) {
      if(!is.null(model)) {
        
        if(any(c('prior','prior_post') %in% output)) {
          # extract name of distribution for node
          distn = model$getDistribution(node)
          
          # get names of parameters for distribution
          params.names = names(model$getDimension(node, 
                                                  includeParams = TRUE))[-1]
          
          # get values of parameters for distribution
          params.values = sapply(params.names, function(param) {
            model$getParam(node = node, param = param)
          })
          
          # ensure named vector matches the extracted parameters
          names(params.values) = params.names
          
          # base string for evaluating distribution
          base_dist = getDistributionInfo(distn)
          
          # density function
          dfn = eval(
            expr = parse(
              text = paste('function(x) ', base_dist$densityName, '(x, ',
                           paste(base_dist$reqdArgs, collapse = ', '), ')', 
                           sep = '')
            ),
            envir = as.list(params.values)
          )
          
          if(base_dist$pqAvail) {
            # quantile function
            qfn = eval(
              expr = parse(
                text = paste('function(x) q', 
                             substr(base_dist$densityName, 2, 
                                    nchar(base_dist$densityName)), 
                             '(x, ', paste(base_dist$reqdArgs, collapse = ', '),
                             ')', sep = '')
              ),
              envir = as.list(params.values)
            )
            
            # bounds for 95% equal-tailed interval around prior
            qBounds = qfn(c(.01, .99))
          }
          
        }
        
        if('prior' %in% output) {
          # plot prior distribution
          res$prior = ggplot(data.frame(x = qBounds), aes(x = x)) + 
            stat_function(fun = dfn) + 
            theme_few() + 
            theme(panel.border = element_blank()) + 
            xlab(node) + 
            ylab('Density')
        }
        
        if('prior_post' %in% output) {
          
          # extract posterior samples
          post = samples[-burn, node]
          
          # compute posterior summaries
          hpd = HPDinterval(mcmc(post))
          ess = round(effectiveSize(mcmc(post)))
          
          # posterior density estimate, and indices within HPD interval 
          post.density = density(post)
          dens.inds = which(post.density$x >= hpd[1] & post.density$x <= hpd[2])
          
          # plot prior vs posterior comparison
          res$prior_post = ggplot(data.frame(x = post), aes(x = x)) + 
            geom_ribbon(mapping = aes(x = x, ymin = 0, ymax = d),
                        data = data.frame(x = post.density$x, 
                                          d = post.density$y)[dens.inds,],
                        inherit.aes = FALSE, alpha = .125) +
            stat_density(geom = 'line') + 
            stat_function(fun = dfn, lty = 2) + 
            theme_few() + 
            theme(panel.border = element_blank()) + 
            xlab(node) + 
            ylab('Density') + 
            ggtitle(label = 'Prior (dotted) vs. Posterior (solid)', 
                    subtitle = paste('with 95% HPD (shaded); ESS:', 
                                     prettyNum(ess, big.mark = ',')))
        }
        
        # extract posterior samples
        post = samples[-burn, node]
        
        if('trace' %in% output) {
          res$trace = ggplot(data.frame(x = (1:nrow(samples))[-burn], 
                                        y = post), 
                             aes(x = x, y = y)) +
            geom_line() + 
            xlab('Sample') + 
            ylab(node) + 
            theme_few() + 
            theme(panel.border = element_blank())
        } 
        
        if('post' %in% output) {
          
          # compute posterior summaries
          hpd = HPDinterval(mcmc(post))
          ess = round(effectiveSize(mcmc(post)))
          
          # posterior density estimate, and indices within HPD interval 
          post.density = density(post)
          dens.inds = which(post.density$x >= hpd[1] & post.density$x <= hpd[2])
          
          # plot prior vs posterior comparison
          res$post = ggplot(data.frame(x = post), aes(x = x)) + 
            geom_ribbon(mapping = aes(x = x, ymin = 0, ymax = d),
                        data = data.frame(x = post.density$x, 
                                          d = post.density$y)[dens.inds,],
                        inherit.aes = FALSE, alpha = .125) +
            stat_density(geom = 'line') + 
            theme_few() + 
            theme(panel.border = element_blank()) + 
            xlab(node) + 
            ylab('Density') + 
            ggtitle(label = 'Posterior', 
                    subtitle = paste('with 95% HPD (shaded); ESS:', 
                                     prettyNum(ess, big.mark = ',')))
        }
      } else {
        stop('Input model is NA; cannot extract prior distribution')
      }
    }
    
    res
  })
  
  names(res) = tgt
  res
}
```

  
  
### Posterior densities

```{r sigma}
library(ggpubr)

pl = distn_info(pattern = '^sigma', model = droneLengths, 
                samples = samples, burn = burn, output = 'prior_post')

pl[[1]]$prior_post + xlab(expression(sigma[Barometer]))
pl[[2]]$prior_post + xlab(expression(sigma[Laser]))
pl[[3]]$prior_post + xlab(expression(sigma[Pixels]))
```

### Transformed and latent posteriors

```{r L}
# 
print('ESS for latent altitudes')
tgt = grep(pattern = '^a', x = samples.names, value = TRUE)
m = mcmc(samples[-burn, tgt, drop = FALSE])
summary(effectiveSize(m))

print('ESS for estimated lengths')
tgt = grep(pattern = paste('L\\[(',
                           paste(nim_pkg$consts$L_unknown_inds, collapse = '|'),
                           ')\\]', sep = ''),
           x = samples.names, value = TRUE)
m = mcmc(samples[-burn, tgt, drop = FALSE])
summary(effectiveSize(m))
```

### Posterior length estimates {.tabset}

```{r length_posteriors, results = 'asis', echo = FALSE}
for(tg in tgt) {

  cat('####', tg, ' \n\n')

  tg.info = nim_pkg$maps$L %>% dplyr::filter(NodeName == tg)
  cat('Subject: ', tg.info$Subject, ' \n\n')
  cat('Measurement: ', tg.info$Measurement, ' \n\n')

  tg_pattern = tg
  tg_pattern = gsub(pattern = '\\[', replacement = '\\\\[', x = tg_pattern)
  tg_pattern = gsub(pattern = '\\]', replacement = '\\\\]', x = tg_pattern)

  pl = distn_info(pattern = tg_pattern, model = droneLengths,
                  samples = samples, burn = burn, output = 'post')

  print(pl[[1]]$post)

  cat(' \n\n')
}

```



### Length Prediction Tables
```{r}
library(kableExtra)
loadd(Mns)  # load expected whale measurements 
loadd(length_samples_mns)  # load predicted whale lengths

mn_ls <- readRDS(file.path("..", "..", "output", "mcmc", "length_samples_mns.rds"))  # read rds file of predicted lengths
IDlist <- names(mn_ls)  # extract AIDs
fulldf <- mn_ls         # save mn_ls to be used in HPD function below

# make a list of names for columns of summary stats table
summcols.names <- c("TL", "TL.05.00..Width", "TL.10.00..Width", "TL.15.00..Width", "TL.20.00..Width", "TL.25.00..Width", "TL.30.00..Width", "TL.35.00..Width", "TL.40.00..Width", "TL.45.00..Width", "TL.50.00..Width", "TL.55.00..Width", "TL.60.00..Width", "TL.65.00..Width", "TL.70.00..Width", "TL.75.00..Width", "TL.80.00..Width", "TL.85.00..Width", "TL.90.00..Width", "TL.95.00..Width")
                    

# make an empty dataframe to fill with the summary stats
summstats.df <- data.frame()

#make a function that calculates the HPD interval and mean of each summary stats column, then adds those values to dataframe
HPDfunc <- function(subdf,tempdf,scol,cname){
  ess = round(effectiveSize(mcmc((subdf[[scol]]))))  # calculate effective sample size (ess) for each column
  tempdf[[paste(cname,".ESS",sep="")]] <- ess    # add column for ess that length column
  hpd = HPDinterval(mcmc(subdf[[scol]])) #calculate HPD interval of column
  tempdf[[paste(cname,".mean",sep="")]] <- mean(subdf[[scol]]) #calculate mean and add column with mean to each length column
  tempdf[[paste(cname,".lower",sep="")]] <- hpd[1] #add column for HPD lower bound value to each length column
  tempdf[[paste(cname,".upper",sep="")]] <- hpd[2] #add columns for HPD upper bound value to each length column
  return(tempdf) #return dataframe with those added columns
}


#loops through list of IDs (loop through each individual's data frame)
for (x in 1:length(IDlist)){
  xx <- IDlist[x] #pull ID name from list
  AID <- xx #save ID as variable
  tempdf <- data.frame(AID) #make empty dataframe with one column containing animal ID, the HPD func will add to this df
  subdf <- fulldf[[xx]] #pull that dataframe of the individual from the named list of dataframes 
  #now we'll loop through the names of the columns we need summary stats for
  for (y in 1:length(summcols.names)){
    scol <- summcols.names[y] #pull the name of the length column
    cname <- summcols.names[y] #pull the name of the column formatted to become the name of the summary stats header
    tempdf <- HPDfunc(subdf,tempdf,scol,cname) #run the HPD function on that column
  }
  summstats.df <- rbind(summstats.df,tempdf) #add the row of summary stats for this individual to the main dataframe
}


# Before linking 'L[#]' to AID/measurements, need to rearrange expected whale measurements dataframe (Mns)
mns_piv <- Mns %>% pivot_longer(cols = starts_with("TL"),
                 names_to = "Measurement", values_to = "Exp_Length") %>%
  mutate(BaroAlt_LHt = round(BaroAlt + Launch_Ht, 2),
         Exp_Length = round(Exp_Length,2)) %>%
  select("AID", "Image", "BaroAlt_LHt", "LaserAlt", "Measurement", "Exp_Length")

# Link "L[#]" to AID + Meas 
mns_rdy <-  nim_pkg$maps$L %>% select(!"Estimated") %>% rename(AID = Subject) %>% 
  left_join(mns_piv, by = c("AID", "Measurement"))

# Now link predicted measurements w/ expected measurements based on AID and measurement
expL <- summstats.df %>% pivot_longer(col = contains("mean"), names_to = "prediction", values_to = "pred_Length") %>% 
  mutate(
    Measurement = sub(".mean", "", prediction),
    pred_Length = round(pred_Length, 2),
  )  %>% select("AID", "Measurement", "pred_Length") %>% left_join(mns_rdy, by = c("AID", "Measurement"))


# Now create similar variables for  HPD lower, HPD upper, & ESS
hpd.low <- summstats.df %>% pivot_longer(col = contains("lower"), names_to = c("prediction"), values_to = "HPD_lower") %>% 
  mutate(
    Measurement = sub(".lower", "", prediction),
    HPD_lower = round(HPD_lower, 2)
  )  %>% select("AID", "Measurement", "HPD_lower")

hpd.up <- summstats.df %>% pivot_longer(col = contains("upper"), names_to = c("prediction"), values_to = "HPD_upper") %>% 
  mutate(
    Measurement = sub(".upper", "", prediction),
    HPD_upper = round(HPD_upper, 2)
  )  %>% select("AID", "Measurement", "HPD_upper")

ess_x <- summstats.df %>% pivot_longer(col = contains("ESS"), names_to = c("prediction"), values_to = "ESS") %>% 
  mutate(
    Measurement = sub(".ESS", "", prediction),
  )  %>% select("AID", "Measurement", "ESS")


# Now merge HPD lower, HPD upper, & ESS and then merge w/ L[#] and TLs
others <- hpd.low %>% left_join(hpd.up, by = c("AID", "Measurement")) %>% 
  mutate(
    HPD_width = HPD_upper - HPD_lower
    ) %>%
  left_join(ess_x, by = c("AID", "Measurement"))

# Merge w/ the predicted lengths, HPDs, ESS w/ expected lengths
tbl <- expL %>% left_join(others, by =c ("AID", "Measurement")) %>% mutate(
  length_diff = round(Exp_Length - pred_Length, 2)
  ) 

# Summary table for report
tbl_report <- tbl %>%
  select(
  "AID", "Image", "Measurement", "NodeName", "BaroAlt_LHt", "LaserAlt", "Exp_Length", "pred_Length", "length_diff", "HPD_width", "ESS")


library(DT) 

# summary of predicted and expected lengths
print("Summary table: Expected and Predicted lengths")
summary(tbl_report[,5:11]) %>% kbl %>% kable_paper("hover")


# https://haozhu233.github.io/kableExtra/awesome_table_in_html.html

# table of predicted and expected lengths
print("Expected and Predicted lengths")
datatable(tbl_report)

```

### Plots
```{r}

# Histogram
ggplot(data = tbl, aes(x = length_diff)) + geom_histogram(binwidth = 0.05, position = 'identity') + xlab("difference between expected and predicted length")


# TL and HPD widths
print("95% HPD Intervals")
tbl %>% dplyr::filter(Measurement == "TL") %>%  ggplot(aes(x = pred_Length, y = HPD_width)) + 
  geom_point() + ylab("95% HPD Interval width (m)") + xlab("TL (m)") + theme(legend.position="bottom")



# kable tables: https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
print("Total Length Predictions")
datatable(tbl %>% dplyr::filter(Measurement == "TL") %>% select(c(AID, Measurement, NodeName, Image, BaroAlt_LHt, LaserAlt, Exp_Length, HPD_lower, pred_Length, HPD_upper, HPD_width, length_diff, ESS)))


```


##### Altitudes
```{r}

# Altitude
ggplot(data = tbl, aes(x = LaserAlt, y = BaroAlt_LHt)) + geom_point() + geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + ggtitle("Expected Altitudes") +
  xlab("Laser altitude (m)") + ylab("Barometer altitude (m)") 


## predicted altitudes w/ expected altitudes
aa = grep(pattern = '^a', x = samples.names, value = TRUE)
maa = mcmc(samples[-burn, aa, drop = FALSE])
pred_a <- as.data.frame(HPDinterval(maa))
pred_a$mean <- colMeans(maa)
pred_a$exp_baro <- nim_pkg$data$a_baro
pred_a$exp_laser <- nim_pkg$data$a_laser
pred_a$hpd_width <- round(pred_a$upper - pred_a$lower, 2)


# writing a temporary csv only to import back in to make "a[#]" an actual column that can be used to link w/ other data
write.csv(pred_a, "pred_alts-temp.csv")
alts <- read.csv("pred_alts-temp.csv")


loadd(APE_images)
loadd(Mns_images)

alts_key <- alts %>% mutate(
      a = X,
      pred_altitude = mean,
      HPD_lower = lower, 
      HPD_upper = upper,
      HPD_width = hpd_width
  ) %>%
  left_join(rbind(APE_images, Mns_images) %>% 
                                 mutate(
                                   exp_laser = AltitudeLaser), 
                               by = "exp_laser"
                               ) %>% 
  select(c("a", "Image", "pred_altitude", "HPD_lower", "HPD_upper", "HPD_width", "exp_baro", "exp_laser"))
```

##### Training vs. testing altitudes 
```{r}
# Altitudes and HPD widths


# add a distinction for test vs. train images
loadd(APE_pixels)
alts_key$img_ty <- "test_img"
alts_key[1:nrow(APE_pixels),]$img_ty <- "train_img"


# expected barometer vs. predicted altitude
ggplot(data = alts_key, aes(x = exp_baro, y = pred_altitude, ymin = HPD_lower, ymax = HPD_upper, color = img_ty)) + geom_pointrange() + 
  geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + 
  xlab("expected barometer altitude (m)") + 
  ylab("predicted altitude (m)") + ggtitle("Expected vs. Predicted: barometer") 

# expected laser vs. predicted altitude
ggplot(data = alts_key, aes(x = exp_laser, y = pred_altitude, ymin = HPD_lower, ymax = HPD_upper, color = img_ty)) + geom_pointrange() + 
  geom_smooth(method = "lm", alpha = 0.50, lty = 2, color = "lightblue") + 
  xlab("expected laser altitude (m)") + 
  ylab("predicted altitude (m)") + ggtitle("Expected vs. Predicted: laser") 


### why does the HPD width plot have that pattern?
# HPD widths
print("95% HPD Intervals")
ggplot(data = alts_key, aes(x = pred_altitude, y = HPD_width, color = img_ty)) + 
  geom_point() + ylab("95% HPD Interval width (m)") + xlab("altitude (m)") + ggtitle("HPD widths: altitude") + 
  theme(legend.position="bottom")


alts_key %>% dplyr::filter(is.na(exp_baro)) %>% ggplot(aes(x = HPD_width)) + geom_histogram() + ggtitle("Barometer NA values")
alts_key %>% dplyr::filter(is.na(exp_laser)) %>% ggplot(aes(x = HPD_width)) + geom_histogram() + ggtitle("Laser NA values")


alts_key$alt_quality <- "baro&laser"
test <- rbind(alts_key %>% dplyr::filter(!is.na(exp_laser) & !is.na(exp_baro)), alts_key %>% dplyr::filter(is.na(exp_laser)) %>% mutate(alt_quality = "Laser NA"), alts_key %>% dplyr::filter(is.na(exp_baro)) %>% mutate(alt_quality = "Barometer NA")) 

## need to parce out true training data
ggplot(data = test, aes(x = pred_altitude, y = HPD_width, color = alt_quality, shape = img_ty)) + 
  geom_point() + ylab("95% HPD Interval width (m)") + xlab("altitude (m)") + ggtitle("HPD widths: altitude") + 
  theme(legend.position="bottom") 

```






### Altitude Prediction Tables
```{r}

# Summary of altitudes
print("Summary Table: Expected and Predicted altitudes")
knitr::kable(summary(alts_key), format="markdown")  


# table and summary 
print("Expected and Predicted altitudes")
datatable(alts_key)

```


### Traceplots

```{r sigma_trace, eval = TRUE}
library(ggpubr)

pl = distn_info(pattern = '^sigma\\[[123]\\]', model = droneLengths, 
                samples = samples, burn = burn, output = 'trace')

pl[[1]]$trace + ylab(expression(sigma[Barometer]))
pl[[2]]$trace + ylab(expression(sigma[Laser])) 
pl[[3]]$trace + ylab(expression(sigma[Pixels]))
```